<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.15">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Hyc3z&#39;s blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Hyc3z&#39;s blog Atom Feed"><title data-react-helmet="true">Spark 基础知识 | Hyc3z&#x27;s blog</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://hyc3z.github.io/docs/big-data/spark"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Spark 基础知识 | Hyc3z&#x27;s blog"><meta data-react-helmet="true" name="description" content="Spark"><meta data-react-helmet="true" property="og:description" content="Spark"><link data-react-helmet="true" rel="icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://hyc3z.github.io/docs/big-data/spark"><link data-react-helmet="true" rel="alternate" href="https://hyc3z.github.io/docs/big-data/spark" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://hyc3z.github.io/docs/big-data/spark" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.98273567.css">
<link rel="preload" href="/assets/js/runtime~main.05a31bdb.js" as="script">
<link rel="preload" href="/assets/js/main.95f6b553.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">Hyc3z&#x27;s blog</b></a><a class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Notes</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/hyc3z" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>Hyc3z<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_Pssr toggle_TdHA toggleDisabled_jDku"><div class="toggleTrack_SSoT" role="button" tabindex="-1"><div class="toggleTrackCheck_XobZ"><span class="toggleIcon_eZtF">🌜</span></div><div class="toggleTrackX_YkSC"><span class="toggleIcon_eZtF">🌞</span></div><div class="toggleTrackThumb_uRm4"></div></div><input type="checkbox" class="toggleScreenReader_JnkT" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_P2Lg"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_RiI4" type="button"></button><aside class="theme-doc-sidebar-container docSidebarContainer_rKC_"><div class="sidebar_CW9Y"><nav class="menu thin-scrollbar menu_SkdO"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/ai/artificial_intelligence_in_finance">ai</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/cpp/pcal">cpp</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active hasHref_VCh3" aria-current="page" href="/docs/big-data/hadoop">big-data</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/big-data/hadoop">Hadoop 基础知识</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/big-data/kafka">Kafka &amp; Zookeeper 基础知识</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/big-data/spark">Spark 基础知识</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/blockchain/Chainlink">blockchain</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/cryptography/overview">cryptography</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/data/data_science">data</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/database/Oracle">database</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/design/ddd">design</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/finance/principles_of_economics">finance</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link hasHref_VCh3" href="/docs/git/">git</a><button aria-label="Toggle the collapsible sidebar category &#x27;git&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/graph/graph-database">graph</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/music/Theory">music</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/ts_js/eventloop">ts_js</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/python/re">python</a></div></li></ul></nav></div></aside><main class="docMainContainer_TCnq"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_DM6M"><div class="docItemContainer_vinB"><article><div class="tocCollapsible_jdIR theme-doc-toc-mobile tocMobile_TmEX"><button type="button" class="clean-btn tocCollapsibleButton_Fzxq">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Spark 基础知识</h1></header><h4 class="anchor anchorWithStickyNavbar_mojV" id="spark">Spark<a class="hash-link" href="#spark" title="Direct link to heading">​</a></h4><p><img alt="Intermittent iteration of reads and writes between map and reduce computations" src="/assets/images/lesp_0101-644e0335ae96b94ae72cc6f638bc6c7b.png" width="1322" height="206"></p><p>Spark的诞生，源于Hadoop的几个问题：</p><p>1、管理较难</p><p>这个在学习Hadoop的时候就感受到了，一大堆参数需要调，非常麻烦</p><p>2、MapReduce API</p><p>需要一大堆模板文件和代码，而且异常处理很难。</p><p>3、中间步骤有太多落盘</p><p>诚然保存中间结果到磁盘，可以提升可靠性，但大大降低了速度和性能。</p><p>4、难以适应多种多样的数据需求</p><p>并不是所有任务都可以拆解为Map和Reduce，有些任务比如AI，它当然也可以算作一个Reduce，但这个Reducer该何其地复杂，才能完成哪怕一步的计算。如果是AI训练任务等，则更加困难。</p><p>5、缺乏交互性</p><p>所有的任务都被预先处理好进入Hadoop集群处理，缺乏交互性</p><h5 class="anchor anchorWithStickyNavbar_mojV" id="spark-诞生">Spark 诞生<a class="hash-link" href="#spark-诞生" title="Direct link to heading">​</a></h5><p>Spark 最早由UC Berkeley 的一些研究人员开发，他们认为MapReduce不够效率，而且过于复杂。所以Spark的理念就是：</p><p>Simpler，Faster and Easier</p><p>早期的Spark就已经能够达到Hadoop MapReduce 10~20倍的性能。</p><p>今天已经能达到100倍性能</p><p>Apache Spark—a unified computing engine and set of libraries for big data</p><h5 class="anchor anchorWithStickyNavbar_mojV" id="unified">Unified<a class="hash-link" href="#unified" title="Direct link to heading">​</a></h5><p>Spark被设计为支持各种各样的数据分析任务，比如：</p><ul><li>Simple data</li><li>SQL query</li><li>ML/AI</li><li>Streaming computation</li></ul><p>这些操作都是使用同一个计算引擎，同一套API。</p><p>Spark的API内部还有一些优化，比如先用SQL query取数据，再调用Spark ML Library，</p><p>Spark的引擎会对这些步骤进行合并、优化，减少访问数据的次数来提升性能。</p><h5 class="anchor anchorWithStickyNavbar_mojV" id="computing-engine">Computing Engine<a class="hash-link" href="#computing-engine" title="Direct link to heading">​</a></h5><p>Spark的定义明确说是一个 computing engine，也就是说它是不包括数据的存储和落盘的部分的。你可以在Azure Storage, Amazon S3, Hadoop HDFS, Kafka, Cassandra等数据载体上用Spark，无论是文件系统，数据库还是消息队列。</p><p>相比之下，Hadoop既有计算引擎（MapReduce）又有存储系统（HDFS），使得二者紧紧耦合，难以选择其他的系统（虽然理论上Hadoop也确实可以跑在本地文件系统，S3等之上）。</p><h5 class="anchor anchorWithStickyNavbar_mojV" id="libraries">Libraries<a class="hash-link" href="#libraries" title="Direct link to heading">​</a></h5><p>提供一个统一的API接口来完成通用的数据分析任务。</p><p>Spark除了内部支持的标准库，还有大量第三方库。</p><p>一些知名的Library：</p><ul><li>Spark SQL</li><li>MLlib</li><li>Spark Streaming/Structured Streaming</li><li>GraphX</li></ul><h5 class="anchor anchorWithStickyNavbar_mojV" id="spark-application">Spark Application<a class="hash-link" href="#spark-application" title="Direct link to heading">​</a></h5><p>包含Driver process和Executors</p><p><img alt="image" src="/assets/images/spdg_0201-78b411902419c7fb27042973ee892686.png" width="789" height="539"></p><p><img alt="Spark components communicate through the Spark driver in Spark’s distributed architecture" src="/assets/images/lesp_0202-c291fe7b2a6597bb49056f64c4bd91c1.png" width="1344" height="725"></p><p>Spark Driver负责控制用户交互和协调executors之间的工作。</p><p>Executors负责具体被分配到的工作。</p><p>每个Executor执行的，就是Spark Job。</p><p><img alt="Spark driver creating one or more Spark jobs" src="/assets/images/lesp_0203-325c151502308130ea846aeeb07a9e15.png" width="538" height="363"></p><p>每个Job包含多个Stage，每个Stage包含多个Task</p><p><img alt="Spark stage creating one or more tasks to be distributed to executors" src="/assets/images/lesp_0205-309e3298ad00f0d44c4efbbdf5be5739.png" width="1300" height="369"></p><h5 class="anchor anchorWithStickyNavbar_mojV" id="cluster-mode">Cluster Mode<a class="hash-link" href="#cluster-mode" title="Direct link to heading">​</a></h5><p><img alt="image" src="/assets/images/spdg_1502-d269753ef2f36d3f511fd78dee997476.png" width="603" height="460"></p><p>Cluster Mode 是最常见的提交任务的方式：</p><p>用户提交JAR/Python Script/xx Script -&gt; Cluster manager在Node上启动Driver process和Executor process</p><h5 class="anchor anchorWithStickyNavbar_mojV" id="client-mode">Client Mode<a class="hash-link" href="#client-mode" title="Direct link to heading">​</a></h5><p><img alt="image" src="/assets/images/spdg_1503-81bbbd5aa6647105e04db6752bcf916d.png" width="669" height="608"></p><p>和Cluster Mode 不同的地方在于，Client 所在的机器（也就是任务提交的机器）会维护driver process，然后cluster manager维护executor process。这些机器也被称为Gateway machines或Edge nodes，是可以处在集群之外的。</p><h5 class="anchor anchorWithStickyNavbar_mojV" id="local-mode">Local Mode<a class="hash-link" href="#local-mode" title="Direct link to heading">​</a></h5><p>学习时用，在本地运行</p><h5 class="anchor anchorWithStickyNavbar_mojV" id="lifecycle">Lifecycle<a class="hash-link" href="#lifecycle" title="Direct link to heading">​</a></h5><p><img alt="image" src="/assets/images/spdg_1504-6f0730af1791e59be87dbda74927072f.png" width="835" height="608"></p><p>首先客户提交任务，为Spark Driver Process申请资源。然后Cluster Manager会将Driver放在集群中的某一个node上。Driver开始执行代码，代码中包含SparkSession的初始化逻辑，会和集群交互来在节点中分布Executor进程。Driver负责调度，Workers内部自主通信。</p><p>一个Spark Job包含多个Stage，通常Spark会试着让同一个Stage执行更多的任务。通常来说，应该让任务的分区数量大于集群的Executor数量，来提升效率</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">spark.conf.set(&quot;spark.sql.shuffle.partitions&quot;, 50)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Pipelining就相对好理解，比如先执行map，再执行filter，在执行map，这三个操作在Spark中会被优化在一个stage中完成，中途不落盘而是在内存中计算，来提升效率。</p><h5 class="anchor anchorWithStickyNavbar_mojV" id="dataframe">DataFrame<a class="hash-link" href="#dataframe" title="Direct link to heading">​</a></h5><p>DataFrame就是一个类似表格的数据结构，有列与行。</p><p>Spark的DataFrame和原生的区别在于，能够跨多个节点，而不是在单一节点上。</p><p>可以将Pandas的df转为Spark的df</p><h5 class="anchor anchorWithStickyNavbar_mojV" id="partitions">Partitions<a class="hash-link" href="#partitions" title="Direct link to heading">​</a></h5><p>每个Partition包含总数据中的若干行子数据，存在一台物理机器上。</p><p>如果只有一个Partition，或者只有一个Executor，都无法并行完成任务，只有多Partition多Executor才能真正并行</p><p>这部分分区不需要人为指定，但是有底层API</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="transformations">Transformations<a class="hash-link" href="#transformations" title="Direct link to heading">​</a></h4><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain"># in Python</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">divisBy2 = myRange.where(&quot;number % 2 = 0&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="narrow-dependencynarrow-transformation">Narrow dependency(Narrow Transformation)<a class="hash-link" href="#narrow-dependencynarrow-transformation" title="Direct link to heading">​</a></h5><p><img alt="image" src="/assets/images/spdg_0204-2bd800704fe2ff38e1cae57a503b50f6.png" width="435" height="510"></p><p>对任意一个输入，只有一个输出，称为Narrow Dependency。对这样的操作，Spark会进行Pipelining，所有操作都在内存中进行。</p><h5 class="anchor anchorWithStickyNavbar_mojV" id="wide-dependency">Wide Dependency<a class="hash-link" href="#wide-dependency" title="Direct link to heading">​</a></h5><p><img alt="image" src="/assets/images/spdg_0205-8c3dfa95316513ee42a9f27c9c3c016a.png" width="380" height="510"></p><p>对Shuffle操作，Spark会在集群内部交换Partition，结果会写入磁盘。</p><h5 class="anchor anchorWithStickyNavbar_mojV" id="actions">Actions<a class="hash-link" href="#actions" title="Direct link to heading">​</a></h5><ul><li>Actions to view data in the console</li><li>Actions to collect data to native objects in the respective language</li><li>Actions to write to output data sources</li></ul><p>如：取得一个DataFrame的记录条数</p><p><code>divisBy2.count()</code></p><p>一些Transformation和Actions：</p><table><thead><tr><th align="left">Transformations</th><th align="left">Actions</th></tr></thead><tbody><tr><td align="left"><code>orderBy()</code></td><td align="left"><code>show()</code></td></tr><tr><td align="left"><code>groupBy()</code></td><td align="left"><code>take()</code></td></tr><tr><td align="left"><code>filter()</code></td><td align="left"><code>count()</code></td></tr><tr><td align="left"><code>select()</code></td><td align="left"><code>collect()</code></td></tr><tr><td align="left"><code>join()</code></td><td align="left"><code>save()</code></td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="lazy-evaluation">Lazy Evaluation<a class="hash-link" href="#lazy-evaluation" title="Direct link to heading">​</a></h5><p>在对数据进行操作时，并不立即修改，而是建立一个plan，包含多个操作。</p><p>直到最后，编译这个plan，然后优化后再一起执行。</p><p>比如Predicate Pushdown，这类在数据库经常用到的优化，就会在这个时候做，大大加快处理速度。</p><p><img alt="Lazy transformations and eager actions" src="/assets/images/lesp_0206-b26e0266a9ab6818ae91bcfa2e9b381c.png" width="1220" height="490"></p><p><img alt="image" src="/assets/images/spdg_0209-8aa6a14ed6dd30fba8d6b7d8f9ca107c.png" width="1087" height="483"></p><p><img alt="image" src="/assets/images/spdg_0401-1490378de6a7cf7df788d80259012684.png" width="1014" height="523"></p><h5 class="anchor anchorWithStickyNavbar_mojV" id="logical-planning-and-physical-planning">Logical Planning and Physical Planning<a class="hash-link" href="#logical-planning-and-physical-planning" title="Direct link to heading">​</a></h5><p><img alt="image" src="/assets/images/spdg_0402-3baad4b3fdfeb2daa132e6ca4aab80bd.png" width="1250" height="343"></p><p><img alt="image" src="/assets/images/spdg_0403-582bc7da18e81d77923a495225f51b53.png" width="1367" height="449"></p><h4 class="anchor anchorWithStickyNavbar_mojV" id="rdd">RDD<a class="hash-link" href="#rdd" title="Direct link to heading">​</a></h4><p><img alt="image-20220902090236235" src="/assets/images/image-20220902090236235-6e9eb6bc8a6f1b627afc25ab86e1a262.png" width="1452" height="628"></p><p>参考nsdi12-final138.pdf</p><p>In short, an RDD represents an immutable, partitioned collection of records that can be operated on in parallel. Unlike DataFrames though, where each record is a structured row containing fields with a known schema, in RDDs the records are just Java, Scala, or Python objects of the programmer’s choosing.</p><p>High Level API：</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">from pyspark.sql import SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from pyspark.sql.functions import avg</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Create a DataFrame using SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">spark = (SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .builder</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;AuthorsAges&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate())</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Create a DataFrame </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">data_df = spark.createDataFrame([(&quot;Brooke&quot;, 20), (&quot;Denny&quot;, 31), (&quot;Jules&quot;, 30), </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;TD&quot;, 35), (&quot;Brooke&quot;, 25)], [&quot;name&quot;, &quot;age&quot;])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Group the same names together, aggregate their ages, and compute an average</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">avg_df = data_df.groupBy(&quot;name&quot;).agg(avg(&quot;age&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Show the results of the final execution</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">avg_df.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>对应的Low Level RDD操作：</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain"># Create an RDD of tuples (name, age)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dataRDD = sc.parallelize([(&quot;Brooke&quot;, 20), (&quot;Denny&quot;, 31), (&quot;Jules&quot;, 30), </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;TD&quot;, 35), (&quot;Brooke&quot;, 25)])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Use map and reduceByKey transformations with their lambda </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># expressions to aggregate and then compute average</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">agesRDD = (dataRDD</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .map(lambda x: (x[0], (x[1], 1)))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .map(lambda x: (x[0], x[1][0]/x[1][1])))</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>创建RDD</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">spark.range(10).rdd</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="数据类型">数据类型<a class="hash-link" href="#数据类型" title="Direct link to heading">​</a></h5><table><thead><tr><th align="left">Data type</th><th align="left">Value assigned in Scala</th><th align="left">API to instantiate</th></tr></thead><tbody><tr><td align="left"><code>ByteType</code></td><td align="left"><code>Byte</code></td><td align="left"><code>DataTypes.ByteType</code></td></tr><tr><td align="left"><code>ShortType</code></td><td align="left"><code>Short</code></td><td align="left"><code>DataTypes.ShortType</code></td></tr><tr><td align="left"><code>IntegerType</code></td><td align="left"><code>Int</code></td><td align="left"><code>DataTypes.IntegerType</code></td></tr><tr><td align="left"><code>LongType</code></td><td align="left"><code>Long</code></td><td align="left"><code>DataTypes.LongType</code></td></tr><tr><td align="left"><code>FloatType</code></td><td align="left"><code>Float</code></td><td align="left"><code>DataTypes.FloatType</code></td></tr><tr><td align="left"><code>DoubleType</code></td><td align="left"><code>Double</code></td><td align="left"><code>DataTypes.DoubleType</code></td></tr><tr><td align="left"><code>StringType</code></td><td align="left"><code>String</code></td><td align="left"><code>DataTypes.StringType</code></td></tr><tr><td align="left"><code>BooleanType</code></td><td align="left"><code>Boolean</code></td><td align="left"><code>DataTypes.BooleanType</code></td></tr><tr><td align="left"><code>DecimalType</code></td><td align="left"><code>java.math.BigDecimal</code></td><td align="left"><code>DecimalType</code></td></tr></tbody></table><table><thead><tr><th align="left">Data type</th><th align="left">Value assigned in Python</th><th align="left">API to instantiate</th></tr></thead><tbody><tr><td align="left"><code>BinaryType</code></td><td align="left"><code>bytearray</code></td><td align="left"><code>BinaryType()</code></td></tr><tr><td align="left"><code>TimestampType</code></td><td align="left"><code>datetime.datetime</code></td><td align="left"><code>TimestampType()</code></td></tr><tr><td align="left"><code>DateType</code></td><td align="left"><code>datetime.date</code></td><td align="left"><code>DateType()</code></td></tr><tr><td align="left"><code>ArrayType</code></td><td align="left">List, tuple, or array</td><td align="left"><code>ArrayType(dataType, [nullable])</code></td></tr><tr><td align="left"><code>MapType</code></td><td align="left"><code>dict</code></td><td align="left"><code>MapType(keyType, valueType, [nullable])</code></td></tr><tr><td align="left"><code>StructType</code></td><td align="left">List or tuple</td><td align="left"><code>StructType([fields])</code></td></tr><tr><td align="left"><code>StructField</code></td><td align="left">A value type corresponding to the type of this field</td><td align="left"><code>StructField(name, dataType, [nullable])</code></td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="spark-sql">Spark SQL<a class="hash-link" href="#spark-sql" title="Direct link to heading">​</a></h5><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">spark.read.json(&quot;/data/flight-data/json/2015-summary.json&quot;)\</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .createOrReplaceTempView(&quot;some_sql_view&quot;) # DF =&gt; SQL</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">spark.sql(&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">SELECT DEST_COUNTRY_NAME, sum(count)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">FROM some_sql_view GROUP BY DEST_COUNTRY_NAME</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&quot;&quot;&quot;)\</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .where(&quot;DEST_COUNTRY_NAME like &#x27;S%&#x27;&quot;).where(&quot;`sum(count)` &gt; 10&quot;)\</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .count() # SQL =&gt; DF</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p><img alt="When errors are detected using the Structured APIs" src="/assets/images/lesp_0302-da1d1d7e70e476d3a763ebd9973b0e98.png" width="718" height="423"></p><h5 class="anchor anchorWithStickyNavbar_mojV" id="stream-processing">Stream Processing<a class="hash-link" href="#stream-processing" title="Direct link to heading">​</a></h5><p><img alt="Traditional record-at-a-time processing model" src="/assets/images/lesp_0801-994a70bde4841cba56817e8ae5b26da5.png" width="1187" height="690"></p><h5 class="anchor anchorWithStickyNavbar_mojV" id="micro-batchesdstream">Micro Batches(DStream)<a class="hash-link" href="#micro-batchesdstream" title="Direct link to heading">​</a></h5><p><img alt="Structured Streaming uses a micro-batch processing model" src="/assets/images/lesp_0802-57bd392273471943454f74476d516625.png" width="1384" height="515"></p><p>秒级延迟，但大大降低单机处理数据的开销</p><p>Incremental Execution</p><p><img alt="The Structured Streaming programming model: data stream as an unbounded table" src="/assets/images/lesp_0803-5862a2a249b3e3f6d5536a93d058c562.png" width="1248" height="565"></p><p><img alt="Incremental execution of streaming queries" src="/assets/images/lesp_0805-9ade2b02592cbea1176348dbd309b2e7.png" width="1440" height="743"></p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain"># In Python</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">inputDF = (spark</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .readStream</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .format(&quot;kafka&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .option(&quot;kafka.bootstrap.servers&quot;, &quot;host1:port1,host2:port2&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .option(&quot;subscribe&quot;, &quot;events&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .load())</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">streamingQuery = (counts</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .selectExpr(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;cast(word as string) as key&quot;, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;cast(count as string) as value&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .writeStream</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .format(&quot;kafka&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .option(&quot;kafka.bootstrap.servers&quot;, &quot;host1:port1,host2:port2&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .option(&quot;topic&quot;, &quot;wordCounts&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .outputMode(&quot;update&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .option(&quot;checkpointLocation&quot;, checkpointDir)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .start())</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="性能优化">性能优化<a class="hash-link" href="#性能优化" title="Direct link to heading">​</a></h5><p>总体来说，文件最好使用二进制格式存储，而不是csv格式，因为文件最好能够被Split成多个分块来让不同的进程读取。这也牵出了之前Hadoop遇到的同样问题，可分割压缩格式。Zip就是一个典型的无法分割的压缩格式，使用Zip格式意味着只有一个进程能从头到尾读取它的数据，不能从中分段读取。相对来说，gz, bz2, lz4就都有办法可以分割，提升读取效率。</p><p>避免UDF（User-Defined Functions），尤其是在Python和R语言中，脚本语言本身解析速度就有限，尽量使用Structure API</p><h5 class="anchor anchorWithStickyNavbar_mojV" id="stream-processing-1">Stream Processing<a class="hash-link" href="#stream-processing-1" title="Direct link to heading">​</a></h5><p>优势：</p><p>低延迟，增量更新结果，高效率（与Batch processing 相比）</p><p>挑战：</p><p>乱序数据</p><p>复杂状态</p><p>高吞吐</p><p>Exact once</p><p>低延迟</p><h5 class="anchor anchorWithStickyNavbar_mojV" id="structured-streaming">Structured Streaming<a class="hash-link" href="#structured-streaming" title="Direct link to heading">​</a></h5><p>Structured streaming的目的在于复用已有的Batch API（DataFrame, Dataset, SQL），它将每一条新数据当作表中的一条新纪录。</p><p><img alt="image" src="/assets/images/spdg_2101-ef115015d57e5debbbd4a23070648df9.png" width="1182" height="482"></p><p>从Input读取数据，然后写到Sink中。</p><p>最典型的数据存取源就是Apache Kafka。</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="安装">安装<a class="hash-link" href="#安装" title="Direct link to heading">​</a></h4><p><code>brew install scala</code></p><p><code>brew install apache-spark</code></p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/big-data/spark.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_dcUD" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_foO9"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/big-data/kafka"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Kafka &amp; Zookeeper 基础知识</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/blockchain/Chainlink"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Chainlink</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_cNA8 thin-scrollbar theme-doc-toc-desktop"></div></div></div></div></main></div></div></div>
<script src="/assets/js/runtime~main.05a31bdb.js"></script>
<script src="/assets/js/main.95f6b553.js"></script>
</body>
</html>