<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.15">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Hyc3z&#39;s blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Hyc3z&#39;s blog Atom Feed"><title data-react-helmet="true">Deep Learning from Scratch | Hyc3z&#x27;s blog</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://hyc3z.github.io/docs/ai/deep_learning"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Deep Learning from Scratch | Hyc3z&#x27;s blog"><meta data-react-helmet="true" name="description" content="读书笔记 Deep Learning from Scratch，Advanced Deep Learning with Python"><meta data-react-helmet="true" property="og:description" content="读书笔记 Deep Learning from Scratch，Advanced Deep Learning with Python"><link data-react-helmet="true" rel="icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://hyc3z.github.io/docs/ai/deep_learning"><link data-react-helmet="true" rel="alternate" href="https://hyc3z.github.io/docs/ai/deep_learning" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://hyc3z.github.io/docs/ai/deep_learning" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.98273567.css">
<link rel="preload" href="/assets/js/runtime~main.ad57eab9.js" as="script">
<link rel="preload" href="/assets/js/main.878a9392.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">Hyc3z&#x27;s blog</b></a><a class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Notes</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/hyc3z" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>Hyc3z<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_Pssr toggle_TdHA toggleDisabled_jDku"><div class="toggleTrack_SSoT" role="button" tabindex="-1"><div class="toggleTrackCheck_XobZ"><span class="toggleIcon_eZtF">🌜</span></div><div class="toggleTrackX_YkSC"><span class="toggleIcon_eZtF">🌞</span></div><div class="toggleTrackThumb_uRm4"></div></div><input type="checkbox" class="toggleScreenReader_JnkT" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_P2Lg"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_RiI4" type="button"></button><aside class="theme-doc-sidebar-container docSidebarContainer_rKC_"><div class="sidebar_CW9Y"><nav class="menu thin-scrollbar menu_SkdO"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active hasHref_VCh3" aria-current="page" href="/docs/ai/deep_learning">ai</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/ai/deep_learning">Deep Learning from Scratch</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai/interpretable_ai">Interpretable AI</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/big-data/hadoop">big-data</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/cpp/pcal">cpp</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/cryptography/overview">cryptography</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/blockchain/Chainlink">blockchain</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/data/data_science">data</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/design/microservices">design</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/database/Oracle">database</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link hasHref_VCh3" href="/docs/git/">git</a><button aria-label="Toggle the collapsible sidebar category &#x27;git&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/graph/graph-database">graph</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/python/re">python</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/ts/typescript">ts</a></div></li></ul></nav></div></aside><main class="docMainContainer_TCnq"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_DM6M"><div class="docItemContainer_vinB"><article><div class="tocCollapsible_jdIR theme-doc-toc-mobile tocMobile_TmEX"><button type="button" class="clean-btn tocCollapsibleButton_Fzxq">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Deep Learning from Scratch</h1></header><p>读书笔记 Deep Learning from Scratch，Advanced Deep Learning with Python</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="数学的基本知识">数学的基本知识<a class="hash-link" href="#数学的基本知识" title="Direct link to heading">​</a></h4><h5 class="anchor anchorWithStickyNavbar_mojV" id="求导">求导<a class="hash-link" href="#求导" title="Direct link to heading">​</a></h5><p><img alt="image-20220806170634648" src="/assets/images/image-20220806170634648-408b1a5b4ae98b69ac3516ab0ac23046.png" width="472" height="112"></p><h5 class="anchor anchorWithStickyNavbar_mojV" id="链式法则">链式法则<a class="hash-link" href="#链式法则" title="Direct link to heading">​</a></h5><p><img alt="image-20220806170952733" src="/assets/images/image-20220806170952733-de67af7aa5ba60b685f47d7fd6b7148e.png" width="440" height="88"></p><p>例如：f(x) = sinx, g(x) = x^2^+1</p><p>​			f(g(x))&#x27; = sin(x^2^+1)&#x27; </p><p>​			f&#x27;(g(x))g&#x27;(x) = <!-- -->[sin(x^2^+1)]<!-- -->&#x27;* 2x</p><p>​			= 2cos(x^2^+1)x</p><p>附上求导公式表：大一学的，已经忘光光了。</p><p><img alt="image-20220806172048431" src="/assets/images/image-20220806172048431-678ef7dc6193122f8aaad1a158c6596e.png" width="790" height="1028"></p><p>其中，反三角函数是给定比值，求对应的角度，而三角函数是给定角度，求比值。</p><p>tan是对边比邻边，</p><p>cot是邻边比对边，</p><p>sin是对边比斜边，</p><p>cos是邻边比斜边，</p><p>sec是1/cos 也就是斜边比邻边，</p><p>csc是1/sin 也就是斜边比对边。</p><p><img alt="image-20220806172535155" src="/assets/images/image-20220806172535155-f58de02ca2fe1a3f75dc36c59432f867.png" width="766" height="1050"></p><p><img alt="image-20220806172609331" src="/assets/images/image-20220806172609331-cd7f66a499af84a19bda1cdd4bfccb3d.png" width="766" height="1084"></p><p><img alt="image-20220806172704001" src="/assets/images/image-20220806172704001-8963ff0a311b3dc7dd158be74f3580ba.png" width="734" height="404"></p><p>哎，高中数学忘得精光……工作了虽然也用不到，但还是复习一下。</p><p>书里对链式法则的图解也很直观，两个delta相乘。</p><p><img alt="image-20220806172753434" src="/assets/images/image-20220806172753434-a3dacb7b72e43971c7970afb0c459916.png" width="896" height="456"></p><p><img alt="image-20220806173028244" src="/assets/images/image-20220806173028244-c697def2b3549b8e10bc266646e4512c.png" width="698" height="112"></p><p>为什么要复习导数呢？因为，正向传播就是函数嵌套的过程，f~3~(f~2~(f~1~(x)))，</p><p>而反向传播就是对这个嵌套函数进行递归求导。</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="向量乘法">向量乘法<a class="hash-link" href="#向量乘法" title="Direct link to heading">​</a></h4><p><img alt="image-20220807082711659" src="/assets/images/image-20220807082711659-399370ed6a3c55897a33ea1a265c91f9.png" width="400" height="82"></p><p><img alt="image-20220807082829202" src="/assets/images/image-20220807082829202-da52a88fc6c80a8924bbd84bb55e4efa.png" width="366" height="96"></p><h4 class="anchor anchorWithStickyNavbar_mojV" id="概率">概率<a class="hash-link" href="#概率" title="Direct link to heading">​</a></h4><p><img alt="image-20220807083003365" src="/assets/images/image-20220807083003365-5b6bc0f28c2a68fd5a10b8e0d4a34de4.png" width="462" height="110"></p><p><img alt="image-20220807083036432" src="/assets/images/image-20220807083036432-4bdf1167624074adbd9116d5a83c1d85.png" width="598" height="88"></p><p><img alt="image-20220807083202948" src="/assets/images/image-20220807083202948-4bdf1167624074adbd9116d5a83c1d85.png" width="598" height="88"></p><p>贝叶斯公式</p><p><img alt="image-20220807083758133" src="/assets/images/image-20220807083758133-0c4e26caac4a166b28bd8fdf2dafdbab.png" width="828" height="160"></p><p>Variance就是方差</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="正态分布函数">正态分布函数<a class="hash-link" href="#正态分布函数" title="Direct link to heading">​</a></h4><p><img alt="image-20220807084744848" src="/assets/images/image-20220807084744848-30d7e73583647e07c27d4a2ffc466f21.png" width="806" height="264"></p><h4 class="anchor anchorWithStickyNavbar_mojV" id="信息公式">信息公式<a class="hash-link" href="#信息公式" title="Direct link to heading">​</a></h4><p><img alt="image-20220807084903317" src="/assets/images/image-20220807084903317-2f9e9818cb857c890fc52cfa0fc2fd4c.png" width="288" height="56"></p><p>香农的信息熵：</p><p><img alt="image-20220807085009069" src="/assets/images/image-20220807085009069-ea55a25564f76f46ca87610261d037e4.png" width="874" height="236"></p><h5 class="anchor anchorWithStickyNavbar_mojV" id="偏导">偏导<a class="hash-link" href="#偏导" title="Direct link to heading">​</a></h5><p>一个多输入函数：$\sigma$ (f(x,y))的正向和反向传播过程</p><p>(markdown里怎么打希腊字母：<code>$\sigma$</code>)</p><p><img alt="image-20220806173538186" src="/assets/images/image-20220806173538186-c52d4d294520aefd1c14cb4a88eb80ce.png" width="874" height="200"></p><p><img alt="image-20220806173735754" src="/assets/images/image-20220806173735754-4bf0f71696eeb4e3ef9b28f739a35233.png" width="900" height="408"></p><p>多输入函数对x求偏导：</p><p><img alt="image-20220806173840303" src="/assets/images/image-20220806173840303-8087904bf01fc56e118ade7fa561d33d.png" width="724" height="104"></p><p><img alt="image-20220806173900284" src="/assets/images/image-20220806173900284-60ffb0f542ba830a1d58cdc19bf2d608.png" width="262" height="110"></p><p>这个法则很重要，也很简单，它决定了反向传播最后一层（输入层）对某个变量的求导结果是多少。</p><p><img alt="image-20220806221708569" src="/assets/images/image-20220806221708569-b653c62fcb9bab88a3cb3c6240971749.png" width="368" height="110"></p><p>对向量求导：</p><p><img alt="image-20220806221740035" src="/assets/images/image-20220806221740035-6fa2a62678a2e7e55eef45c361133703.png" width="866" height="438"></p><p>相当于对每个元素分别求导</p><p>Deep Learning:</p><p>“Repeatedly feed observations through the model, keeping track of the quantities computed along the way during this “forward pass.”</p><p>Calculate a loss representing how far off our model’s predictions were from the desired outputs or target.</p><p>Using the quantities computed on the forward pass and the chain rule math worked out in Chapter 1, compute how much each of the input parameters  ultimately affects this loss.</p><p>Update the values of the parameters so that the loss will hopefully be reduced when the next set of observations is passed through the model.</p><p>每个Layer 都有Backward和Forward，也就是基本的Dense Layer</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="normalize">Normalize<a class="hash-link" href="#normalize" title="Direct link to heading">​</a></h4><p>其实这个很好理解，因为概率总是在0..1之间的。因此</p><p><img alt="image-20220806223700382" src="/assets/images/image-20220806223700382-b3ff29fc8c0b2fa799cdca99bc588b57.png" width="750" height="280"></p><p>对向量的正则化就是把每个元素除以元素的总和。</p><p>Softmax函数</p><p><img alt="image-20220806223759466" src="/assets/images/image-20220806223759466-9266bea72ffb076f31f542edcd8cc33a.png" width="614" height="258"></p><p>Softmax等于是更加突出了那个极值，放大了标准差。</p><p>Cross Entropy</p><p><img alt="image-20220806223937338" src="/assets/images/image-20220806223937338-b2566caadbc90e460e48b4072a083242.png" width="700" height="82"></p><p>和前面的Softmax结合：</p><p><img alt="image-20220806224158080" src="/assets/images/image-20220806224158080-4db11e9dc8641a8f8cd1dac72bdbfdc1.png" width="878" height="86"></p><p>log消掉，剩下</p><p><img alt="image-20220806224228248" src="/assets/images/image-20220806224228248-917bfc12f3b982c18dfda5d292e54716.png" width="482" height="128"></p><p>是不是很神奇！</p><p>所以SCE loss_grad 就等于 softmax_x -y</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="用relu取代sigmoid来提升训练速度">用ReLU取代Sigmoid来提升训练速度<a class="hash-link" href="#用relu取代sigmoid来提升训练速度" title="Direct link to heading">​</a></h4><p>Sigmoid的最大斜率是0.25，而且在&lt;-2 或&gt;2时，斜率接近0，这样导致更新变量的速度非常慢。</p><p>ReLU就是另一个极端，f(x) = x (x &gt; 0) , 0 (x &lt;= 0).</p><p>它也是符合激活函数的定义的，单调且非线性。</p><p>Leaky ReLU和Tanh</p><p>Tanh是长这样的，值域在(-1,1)</p><p><img alt="image-20220806224737520" src="/assets/images/image-20220806224737520-58793778cb7812c9550b78dbd9f70eae.png" width="792" height="596"></p><p>Leaky ReLU和ReLU不同的地方在 x &lt; 0的地方，Leaky ReLU是y=kx，0 &lt; k &lt; 1.</p><p>此外，还有ReLU6等。</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="momentum">Momentum<a class="hash-link" href="#momentum" title="Direct link to heading">​</a></h4><p><img alt="image-20220806225202215" src="/assets/images/image-20220806225202215-cc6d2e1de7dcfe2d8a68caf60865cc88.png" width="758" height="202"></p><p>每一步更新x的速度不再是固定的，而是根据以往的速度计算得出。</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="learning-rate-decay">Learning Rate Decay<a class="hash-link" href="#learning-rate-decay" title="Direct link to heading">​</a></h4><p>随着Momentum慢慢变小，可能已经到了局部最优或者全局最优，再继续Training也只有很小的改变了。当Final Learning Rate低于阈值时，我们就可以停止训练了。</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="weight-initialization">Weight Initialization<a class="hash-link" href="#weight-initialization" title="Direct link to heading">​</a></h4><p>权重初始化，为什么需要这个呢？因为之前的Tanh，Sigmoid，在input = 0的时候有非常高的斜率。因此输入的权重 = 0并不是什么很好的选择。我们可以初始化权重，来让这种情况得到缓解。</p><p>Glorot初始化：</p><p><img alt="image-20220806225743439" src="/assets/images/image-20220806225743439-d1ae193339f5760a0ee29855f732bb86.png" width="682" height="104"></p><p>给每一层的权重赋值都是根据输入的个数和输出个数决定的。</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="dropout">Dropout<a class="hash-link" href="#dropout" title="Direct link to heading">​</a></h4><p>神经网络为啥不能无脑堆层数呢？因为容易过拟合，陷入局部最优。因此，我们不仅不能无脑堆，还要剪枝，把用不到的神经元关掉。很简单，把它们的权重设为0就好了，这样Forward和Backward都不会经过它。</p><p>同样，其他神经元的权重也会相应调整为Magnitude * (1-p)。</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="convolutional">Convolutional<a class="hash-link" href="#convolutional" title="Direct link to heading">​</a></h4><p>卷积核和矩阵，这个就不记录了，也是一样的矩阵元素相乘相加。</p><p>“The interpretation of each neuron of a fully connected layer is that it detects whether or not a particular combination of the features learned by the prior layer is present in the current observation.</p><p>The interpretation of a neuron of a convolutional layer is that it detects whether or not a particular combination of visual patterns learned by the prior layer is present at the given location of the input image.”</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="1x1卷积">1x1卷积<a class="hash-link" href="#1x1卷积" title="Direct link to heading">​</a></h4><p>可以改变输出的深度，也叫做Bottleneck layer</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="dilated-convolution">Dilated Convolution<a class="hash-link" href="#dilated-convolution" title="Direct link to heading">​</a></h4><p><img alt="image-20220807090017553" src="/assets/images/image-20220807090017553-22c1dae30932d4c043627a124401367d.png" width="898" height="572"></p><h4 class="anchor anchorWithStickyNavbar_mojV" id="guided-back-propagation">Guided back propagation<a class="hash-link" href="#guided-back-propagation" title="Direct link to heading">​</a></h4><p><img alt="image-20220807090305625" src="/assets/images/image-20220807090305625-6e740e4cb57d8529727aaf2a51a70e70.png" width="914" height="612"></p><h4 class="anchor anchorWithStickyNavbar_mojV" id="flatten-layer">Flatten Layer<a class="hash-link" href="#flatten-layer" title="Direct link to heading">​</a></h4><p>把m <em> height </em> width 的 matrix 变成 1 <em> m </em> height * width的vector</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="pooling-layers">Pooling Layers<a class="hash-link" href="#pooling-layers" title="Direct link to heading">​</a></h4><p>池化层，下采样，降分辨率</p><p>从ResNet开始，就很少会用到了，因为会损失信息</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="padding">Padding<a class="hash-link" href="#padding" title="Direct link to heading">​</a></h4><p>处理边缘值的时候，填充0。否则输出尺寸会比原来小。</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="alexnet">AlexNet<a class="hash-link" href="#alexnet" title="Direct link to heading">​</a></h4><p><img alt="image-20220807091902162" src="/assets/images/image-20220807091902162-a0475f6d3f790b10afbc6a387ebdd536.png" width="874" height="592"></p><h4 class="anchor anchorWithStickyNavbar_mojV" id="vgg">VGG<a class="hash-link" href="#vgg" title="Direct link to heading">​</a></h4><p><img alt="image-20220807091934274" src="/assets/images/image-20220807091934274-69f5060efe712fb11bdb58336d4be249.png" width="434" height="728"></p><h4 class="anchor anchorWithStickyNavbar_mojV" id="resnet">ResNet<a class="hash-link" href="#resnet" title="Direct link to heading">​</a></h4><p><img alt="image-20220807092037288" src="/assets/images/image-20220807092037288-ec1345ee69a7372383df69bd325a5af3.png" width="846" height="564"></p><p>ResNet的作者发现，56层的神经网络比20层错误更多。理论上，更深的神经网络即使有很多不激活的神经元，也至少应该达到浅层网络同样的性能。</p><p>于是，残差网络诞生了，图中的四个都称为Residual Block（残差块），右边增加的这条路称为Identity Shourtcut Connection（Skip connection）。这样在正向传播的时候，不仅会传播Learned features，还会传播原始的输入信号，这样网络就可以决定跳过某些层。同时，也用了Padding来解决输出维度的问题。</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="inception">Inception<a class="hash-link" href="#inception" title="Direct link to heading">​</a></h4><p>同一个物体，距离远在图片中就会小，而距离近则会更大。这对神经元产生了挑战，因为神经元的感受野大小是固定的。Inception从输入开始，就在各个Path上进行计算（Towers），每个Path包含不同大小、尺度的Filter等。</p><h5 class="anchor anchorWithStickyNavbar_mojV" id="inception-v1">Inception V1<a class="hash-link" href="#inception-v1" title="Direct link to heading">​</a></h5><p><img alt="image-20220807092949826" src="/assets/images/image-20220807092949826-1ecfb75391a14ce8e06cf622f9acd956.png" width="938" height="452"></p><h5 class="anchor anchorWithStickyNavbar_mojV" id="inception-v2-v3">Inception V2 V3<a class="hash-link" href="#inception-v2-v3" title="Direct link to heading">​</a></h5><p><img alt="image-20220807093030646" src="/assets/images/image-20220807093030646-4ea9048e2aa3248c1160ec33c5f77e21.png" width="918" height="476"></p><h5 class="anchor anchorWithStickyNavbar_mojV" id="inception-v4inception-resnet">Inception V4(Inception Resnet)<a class="hash-link" href="#inception-v4inception-resnet" title="Direct link to heading">​</a></h5><p><img alt="image-20220807093133554" src="/assets/images/image-20220807093133554-a7af04fdf0affc09927eacf338e5f34b.png" width="690" height="568"></p><h4 class="anchor anchorWithStickyNavbar_mojV" id="neural-architecture-search">Neural Architecture Search<a class="hash-link" href="#neural-architecture-search" title="Direct link to heading">​</a></h4><p>让神经网络自行学习神经网络的参数：</p><p><img alt="image-20220807093454848" src="/assets/images/image-20220807093454848-2c052e1c60a4ec35c385ca424f6d2a2b.png" width="878" height="414"></p><p>空白的block就是LSTM Cell</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="picasso-problem">Picasso problem<a class="hash-link" href="#picasso-problem" title="Direct link to heading">​</a></h4><p><img alt="image-20220807093628143" src="/assets/images/image-20220807093628143-4d3801a40425cf61fb6ae861a448d602.png" width="912" height="486"></p><h4 class="anchor anchorWithStickyNavbar_mojV" id="object-detection">Object Detection<a class="hash-link" href="#object-detection" title="Direct link to heading">​</a></h4><p>Sliding window 滑动窗口</p><p>Two-stage：RPN(Region Proposal Network or RoI) 产生Bounding Box候选，供后续检测物体</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="yolo">YOLO<a class="hash-link" href="#yolo" title="Direct link to heading">​</a></h4><p>1、将图片分割成S*S的Cells</p><p>2、对每个Cell进行检测，输出0或多个检测出的物体</p><p>3、Intersection over Union(IoU)<img alt="image-20220807094601274" src="/assets/images/image-20220807094601274-118ed9c520fb3e289af970cfc90a6d4c.png" width="900" height="344"></p><p>选出最大IoU的anchor box，能够代表物体的footprint</p><p>4、Non-maximum Suppression NMS 去除重复的bounding boxes</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="rnn">RNN<a class="hash-link" href="#rnn" title="Direct link to heading">​</a></h4><p>处理Sequence数据</p><p><img alt="image-20220806232231774" src="/assets/images/image-20220806232231774-4be61fd4ba0964d60659a6694f698690.png" width="914" height="620"></p><p>这样的原因在于，我们的输入是序列的数据，我们希望t=1的数据能影响到t=2的结果，而不是相互独立。我们希望把时间这个维度也加进来。</p><p><img alt="image-20220806232748694" src="/assets/images/image-20220806232748694-1142d1c8ae7ec46e5f99dd96ea279290.png" width="902" height="798"></p><p>在t=1的时候，我们把随机初始化的向量输入，若干次迭代后得到一个输出向量。然后，我们把输出向量通过某种方式和下一次输入一起合并，得到t=2的输出，这样一直迭代下去。</p><p><img alt="image-20220807101646130" src="/assets/images/image-20220807101646130-9f7961169783765a09b702465ba373ab.png" width="382" height="108"></p><p><img alt="image-20220806233030708" src="/assets/images/image-20220806233030708-bc5b37c982a8129adb00d1c2a4b4ddf5.png" width="898" height="448"></p><p>每一个RNN Layer都由固定数量的RNN Node组成。RNN的Back propagation会更新上一个time step的数值。</p><p>LSTM Node和Vanilla RNN Node的区别就在于它不仅仅存储了代表序列学习信息的Hidden Value，还存储了一个cell state，来建立Long Short term依赖关系。</p><p>Vanilla RNN的缺陷在于，它只能依赖最近的数据来预测未来，却难以拥有“记忆”，以及对久远数据的权重把握。</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="gru-node">GRU Node<a class="hash-link" href="#gru-node" title="Direct link to heading">​</a></h4><p>Learn to forget</p><p><img alt="image-20220806233828778" src="/assets/images/image-20220806233828778-10c41618016089789eba515ace504d02.png" width="894" height="782"></p><h4 class="anchor anchorWithStickyNavbar_mojV" id="lstm-node">LSTM Node<a class="hash-link" href="#lstm-node" title="Direct link to heading">​</a></h4><p><img alt="image-20220806233903406" src="/assets/images/image-20220806233903406-00a6f3aa020cb6ce610020e0bd93d090.png" width="868" height="706"></p><h4 class="anchor anchorWithStickyNavbar_mojV" id="transfer-learning">Transfer Learning<a class="hash-link" href="#transfer-learning" title="Direct link to heading">​</a></h4><p>“TL is the process of applying an existing trained ML model to a new, but related, problem.”</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="u-net">U-Net<a class="hash-link" href="#u-net" title="Direct link to heading">​</a></h4><p><img alt="image-20220807094955646" src="/assets/images/image-20220807094955646-59cdc202d658162563a60c8fc1574bb7.png" width="924" height="864"></p><p>包含两部分，Encoder和Decoder</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="vae">VAE<a class="hash-link" href="#vae" title="Direct link to heading">​</a></h4><p><img alt="image-20220807095328652" src="/assets/images/image-20220807095328652-dd226d920f73046f037672922c128584.png" width="876" height="866"></p><p>Encoder: 可以有多层hidden layer，将输入接收到网络中</p><p>Decoder: 试图从网络的内部feature重构输入</p><h5 class="anchor anchorWithStickyNavbar_mojV" id="denoising-autoencoder">Denoising Autoencoder<a class="hash-link" href="#denoising-autoencoder" title="Direct link to heading">​</a></h5><p><img alt="image-20220807095544639" src="/assets/images/image-20220807095544639-67bf7e860d2e9e27f54e7a2507c27318.png" width="858" height="498"></p><p>可以用这个Auto encoder来去噪</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="n-gram">N-gram<a class="hash-link" href="#n-gram" title="Direct link to heading">​</a></h4><p><img alt="image-20220807100038638" src="/assets/images/image-20220807100038638-3164941d01252ebdc483eba90b1c56fb.png" width="814" height="326"></p><p><img alt="image-20220807100056539" src="/assets/images/image-20220807100056539-21bcbc3b45ff2e54fe4bf04818157297.png" width="722" height="178"></p><p><img alt="image-20220807100115807" src="/assets/images/image-20220807100115807-c7d5a49e2a2ce73d8f4ccf54e8156a55.png" width="802" height="134"></p><p>维度爆炸问题（curse of dimensionality）：</p><p>n越大，可能性越大，复杂性指数升高</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="cbow">CBOW<a class="hash-link" href="#cbow" title="Direct link to heading">​</a></h4><p><img alt="image-20220807100322330" src="/assets/images/image-20220807100322330-a1c1804652218706ee61fb81a40e1845.png" width="852" height="390"></p><p>分辨出给定context哪个word是最可能出现的。</p><p><img alt="image-20220807101844066" src="/assets/images/image-20220807101844066-90bff722a48b29b503eed16c08d5fa41.png" width="898" height="714"></p><h4 class="anchor anchorWithStickyNavbar_mojV" id="梯度消失和梯度爆炸">梯度消失和梯度爆炸<a class="hash-link" href="#梯度消失和梯度爆炸" title="Direct link to heading">​</a></h4><p><img alt="image-20220807102046971" src="/assets/images/image-20220807102046971-638dd4d1387812ef4fd6263fc6b947c2.png" width="738" height="162"></p><p><img alt="image-20220807102118589" src="/assets/images/image-20220807102118589-6f901258a3c3f07c77eab8b74e9c9e08.png" width="868" height="166"></p><p>W &gt; 1 就会发生梯度爆炸，W &lt; 1 则会梯度消失</p><p>解决方法：LSTM</p><p>Forget Cell和Input Cell</p><p><img alt="image-20220807102730520" src="/assets/images/image-20220807102730520-2420969e9a282071fce596297b32a79b.png" width="896" height="476"></p><p><img alt="image-20220807102906771" src="/assets/images/image-20220807102906771-d3706aedb2aec59eda35ba8be3c3356c.png" width="488" height="92"></p><p>加号圆圈表示线性相加。</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="seq2seq">Seq2seq<a class="hash-link" href="#seq2seq" title="Direct link to heading">​</a></h4><p><img alt="image-20220807103139176" src="/assets/images/image-20220807103139176-6071ac16027cdd4f83617a8f5c9531ab.png" width="880" height="294"></p><p>输入 A,B,C</p><p>输出W,X,Y,Z</p><p>encoder可以用任意RNN，论文里用的LSTM。</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="transformer">Transformer<a class="hash-link" href="#transformer" title="Direct link to heading">​</a></h4><p><img alt="image-20220807103624281" src="/assets/images/image-20220807103624281-f1ba21e7e5c6e686e80ef3f18118c146.png" width="892" height="668"></p><p><img alt="image-20220807103946637" src="/assets/images/image-20220807103946637-1449abe51dbe77b16e2a2622055159c9.png" width="798" height="122"></p><p>和数据库中的Query Key Value对应。</p><p><img alt="image-20220807104132568" src="/assets/images/image-20220807104132568-5c805c088c0f10f341f1d72d447b4aa2.png" width="864" height="108"></p><p><img alt="image-20220807104155498" src="/assets/images/image-20220807104155498-a59d990570ab04a2c7ff2e769cd0f4db.png" width="838" height="1320"></p><h5 class="anchor anchorWithStickyNavbar_mojV" id="transformer-model">Transformer Model<a class="hash-link" href="#transformer-model" title="Direct link to heading">​</a></h5><p><img alt="image-20220807104231298" src="/assets/images/image-20220807104231298-121cb1f820fa0e1a20c29b61b3105f83.png" width="880" height="1022"></p><p>输入是one-hot encoded sequence</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="bert">BERT<a class="hash-link" href="#bert" title="Direct link to heading">​</a></h4><p><img alt="image-20220807104836358" src="/assets/images/image-20220807104836358-244a048d62f7e6386190329bbfc9009f.png" width="920" height="450"></p><p>Masked Language Modeling(MLM)</p><p>80%的情况：把Word替换成MASK</p><p>“my dog is hairy → my dog is <!-- -->[MASK]<!-- -->”</p><p>10%的情况：替换成随机单词</p><p>“my dog is hairy → my dog is apple”</p><p>10%的情况：不变</p><p>“my dog is hairy → my dog is hairy”</p><h5></h5><h5 class="anchor anchorWithStickyNavbar_mojV" id="transformer-attention">Transformer attention<a class="hash-link" href="#transformer-attention" title="Direct link to heading">​</a></h5><p><img alt="image-20220807105611888" src="/assets/images/image-20220807105611888-46553aa72af4107ea9e81af93e36b019.png" width="528" height="218"></p><p>1 表示content based word i 和 word j 的关联性</p><p>2 表示 word i 和 pos j 上的关联性，比如 i 为cream，那么 j 为ice的可能性就很大</p><p>3 2的反面</p><p>4 pos i和pos j的关联性</p><h5 class="anchor anchorWithStickyNavbar_mojV" id="relative-positional-encodings">Relative positional encodings<a class="hash-link" href="#relative-positional-encodings" title="Direct link to heading">​</a></h5><p><img alt="image-20220807110006555" src="/assets/images/image-20220807110006555-6208f9a921c759c71fa8b2840dac88c1.png" width="496" height="178"></p><p>把绝对位置换成了相对位置</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/ai/deep_learning.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_dcUD" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_foO9"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/intro"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">intro</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/ai/interpretable_ai"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Interpretable AI</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_cNA8 thin-scrollbar theme-doc-toc-desktop"></div></div></div></div></main></div></div></div>
<script src="/assets/js/runtime~main.ad57eab9.js"></script>
<script src="/assets/js/main.878a9392.js"></script>
</body>
</html>